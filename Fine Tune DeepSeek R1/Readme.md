# Fine-Tuning DeepSeek-R1-Distill-Llama-8B for Medical QA  

This project fine-tunes **DeepSeek-R1-Distill-Llama-8B** for **Medical Question Answering (MedQA)** using **LoRA adapters**, **PEFT**, and **bnb-4bit quantization**. The training was optimized with **Unsloth** for efficient model loading and **Weights & Biases (WandB)** for performance tracking.  

---

## 📌 Features  
✅ **Efficient Fine-Tuning:** Uses **LoRA + PEFT** for low-cost, memory-efficient tuning.  
✅ **Optimized Inference:** **bnb-4bit quantization** reduces memory usage while maintaining accuracy.  
✅ **Performance Tracking:** Uses **Weights & Biases (WandB)** for experiment monitoring.  
✅ **Fast Training:** Utilizes **Unsloth** for optimized model loading.  

---

## 📂 Dataset Acknowledgment  
This model was fine-tuned on a **Medical QA dataset**, curated from Open Life Science AI.  
I acknowledge the efforts of the dataset creators in making high-quality medical question-answering data available for research and development.  

---

## 📚 Model Acknowledgment  
This project is built upon the **DeepSeek-R1-Distill-Llama-8B** model from **[unsloth](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B)**.  
I acknowledge their contributions to open-source LLM development.  

---

## 🤝 Contributing  
If you have ideas for improving this model or dataset recommendations, feel free to reach out!

